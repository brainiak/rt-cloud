{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook teaches how to use the BIDS Archive, BIDS Incremental, and BIDS Run classes.\n",
    "\n",
    "# Pre-Run Setup\n",
    "\n",
    "\n",
    "## Activating the RT-Cloud Conda Environment in Jupyter\n",
    "\n",
    "After successfully following the Anaconda (conda) setup instructions in the `README` file, be sure to also activate the `rtcloud` kernel in this notebook. To do this, first make sure that you activated the `rtcloud` conda environment in the terminal before you ran the `jupyter notebook` command to launch this notebook. Then, go to Kernel -> Change Kernel, then select the kernel with `rtcloud` in it. If the kernel isn't in the list, execute the following command, restart the notebook server, and try again:\n",
    "```\n",
    "python -m ipykernel install --user --name=rtcloud\n",
    "```\n",
    "After changing kernels, the kernel name in the upper right hand corner of the notebook should look similar to:\n",
    "```\n",
    "Python [conda env:rtcloud]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Setup (run this before executing later code cells): Imports and Constants\n",
    "\"\"\" Add rtCommon to the path \"\"\"\n",
    "import os\n",
    "import sys\n",
    "\n",
    "currPath = os.path.dirname(os.path.realpath(os.getcwd())) # docs\n",
    "rootPath = os.path.dirname(currPath) # project root\n",
    "sys.path.append(rootPath)\n",
    "\n",
    "import io\n",
    "import json\n",
    "import pickle\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from rtCommon.bidsArchive import BidsArchive\n",
    "from rtCommon.bidsCommon import getDicomMetadata, loadBidsEntities\n",
    "from rtCommon.bidsIncremental import BidsIncremental\n",
    "from rtCommon.bidsRun import BidsRun\n",
    "from rtCommon.errors import MissingMetadataError, MetadataMismatchError\n",
    "from rtCommon.imageHandling import convertDicomFileToNifti, readDicomFromFile, readNifti\n",
    "\n",
    "TARGET_DIR = 'dataset'\n",
    "TEMP_NIFTI_NAME = 'temp.nii'\n",
    "DICOM_PATH = 'tests/test_input/001_000013_000005.dcm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "There are a few terms that are important to understand before starting to use the BIDS Archive & BIDS Incremental tutorial.\n",
    "\n",
    "\n",
    "## Understanding BIDS Entities\n",
    "\n",
    "BIDS Entities, referred to later as just 'entities', are used to create the file names of files in a BIDS Archive and describe the file they name. You may already be familiar with common ones, like `subject`, `task`, and `run`. An example of a file name containing BIDS Entities is `sub-01_task-languageproduction_run-01_bold.nii.gz`, which contains the `subject`, `task`, and `run` entities.\n",
    "\n",
    "Most entities are used in a key-value form (`key-value`, e.g., `sub-01`) and have their name and value present wherever they are used. They have three main representations. \n",
    "\n",
    "1. **Entity**: One word, all lowercase. A summary of the entity. (e.g., 'ceagent', 'subject')\n",
    "2. **Full Name**: Up to several words, fully describes the entity. (e.g., 'Contrast Enhancing Agent', 'Subject')\n",
    "3. **File Name Key**: A few characters, used in file names. (e.g., 'ce', 'sub')\n",
    "\n",
    "A few entities and their multiple representations are shown in the table below:\n",
    "\n",
    "| Entity | Full Name | File Name Key |\n",
    "| --- | --- | --- |\n",
    "| ceagent | Contrast Enhancing Agent | ce |\n",
    "| subject | Subject | sub |\n",
    "| session | Session | ses |\n",
    "| run | Run | run |\n",
    "\n",
    "As of this writing, the other valid entities for BIDS and BIDS Derivatives are listed in the BIDS Standard and in JSON files in the PyBids Github repository (https://github.com/bids-standard/pybids/tree/master/bids/layout/config). An easy way to view all valid entities in RT-Cloud, which are loaded from PyBids, is to view the dictionary returned by the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = loadBidsEntities()\n",
    "print(\"Entities:\", entities.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also a few entities with only one representation, which aren't used in a key-value form. Examples include `datatype` (e.g., 'func' or 'anat'), `extension` (e.g., '.nii', '.nii.gz', '.json'), and `suffix` (e.g., 'bold'). See the table below for how these can appear in file naming and archive organization.\n",
    "\n",
    "Together, these entities provide a unique and consistent way to name files and organize the BIDS dataset.\n",
    "\n",
    "#### Exercise: What entities are present in the path `sub-01/func/sub-01_task-languageproduction_run-01_bold.nii.gz`, and what are the entity values? \n",
    "\n",
    "##### Answer: \n",
    "| Entity Name | Value |\n",
    "|---          | ---   |\n",
    "| subject | 01 |\n",
    "| datatype | func |\n",
    "| task | languageproduction |\n",
    "| run | 01 |\n",
    "| suffix | bold |\n",
    "| extension | .nii.gz|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIDS Archive: Opening Existing Dataset\n",
    "\n",
    "Objective: Learn how to create a BIDS Archive pointing to a specific dataset on disk.\n",
    "\n",
    "Procedure:\n",
    "1. Download a small, sample dataset from OpenNeuro to use with `BidsArchive`.\n",
    "2. Open the dataset using `BidsArchive` and print out some summary data about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://openneuro.org/datasets/ds002014/versions/1.0.1/download -- relatively small dataset (<40MB)\n",
    "shutil.rmtree(TARGET_DIR, ignore_errors=True)\n",
    "command = 'aws s3 sync --no-sign-request s3://openneuro.org/ds002014 ' + TARGET_DIR\n",
    "command = command.split(' ')\n",
    "if subprocess.call(command) == 0:\n",
    "    print(\"Dataset successfully downloaded\")\n",
    "else:\n",
    "    print(\"Error in calling download command\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Open downloaded dataset \"\"\"\n",
    "archive = BidsArchive(TARGET_DIR)\n",
    "print('Archive: ', archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIDS Archive: Querying Dataset\n",
    "\n",
    "Objective: Learn how to extract information and files from the `BidsArchive`.\n",
    "\n",
    "Procedure:\n",
    "\n",
    "1. Search for images in the dataset.\n",
    "2. Search for sidecar metadata for the images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Any BIDS entity can be extracted from the archive using getEntity() (e.g., getSubjects(), getRuns(), getTasks())\n",
    "print('Dataset info: Subjects: {subjects} | Runs: {runs} | Tasks: {tasks}\\n'\n",
    "      .format(subjects=archive.getSubjects(), runs=archive.getRuns(), tasks=archive.getTasks()))\n",
    "\n",
    "# Arguments can be passed as keywords or using a dictionary with equivalent results\n",
    "entityDict = {'subject': archive.getSubjects()[0], 'run': archive.getRuns()[0]}\n",
    "imagesUsingDict = archive.getImages(**entityDict)\n",
    "imagesUsingKeywords = archive.getImages(subject=archive.getSubjects()[0], run=archive.getRuns()[0])\n",
    "assert imagesUsingDict == imagesUsingKeywords\n",
    "\n",
    "print('Number of image files associated with Subject {}, Run {}: {}'.format(\n",
    "    entityDict['subject'], entityDict['run'], len(imagesUsingDict)))\n",
    "\n",
    "# Get all images from the functional runs\n",
    "images = archive.getImages(datatype='func')\n",
    "print('Number of functional images: {}'.format(len(images)))\n",
    "\n",
    "# Anatomical images can be retrieved too\n",
    "images = archive.getImages(datatype='anat')\n",
    "print('Number of anatomical images: {}'.format(len(images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No images are returned if matches aren't found\n",
    "subjectName='invalidSubject'\n",
    "images = archive.getImages(subject=subjectName)\n",
    "print('Number of image files associated with Subject \"{}\": {}'.format(subjectName, len(images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen how to get images from an archive, we'll look at how to get metadata for images we've retrieved from the archive.\n",
    "\n",
    "To get metadata for an image, the path to the image file is required. Every `BIDSImageFile` returned from `getImages` has a `path` property you can use to obtain this path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all image files, then create a dictionary mapping each image file's path to its metadata dictionary\n",
    "imageFiles = archive.getImages()\n",
    "metadata = {i.path: archive.getSidecarMetadata(i.path) for i in imageFiles}\n",
    "for path, metaDict in metadata.items():\n",
    "    print('Metadata for:', path, \"is:\\n\", json.dumps(metaDict, indent=4, sort_keys=True), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last piece of data we'll see how to get from an archive is the events file corresponding to a particular scanning run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event files to get can be filtered by entities, as with \n",
    "# getImages and getSidecarMetadata\n",
    "events = archive.getEvents(subject='01', \n",
    "                           task='languageproduction', run=1)\n",
    "\n",
    "# All event files can be retrieved when specifiying no entities\n",
    "events = archive.getEvents()\n",
    "\n",
    "# Event files are returned as BIDSDataFile objects\n",
    "# See the PyBids documentation for more information on those\n",
    "eventsFile = events[0]\n",
    "print('Events file type: ', type(eventsFile))\n",
    "\n",
    "# One method of the BIDSDataFile object returns\n",
    "# a Pandas data frame of the events file\n",
    "eventsDF = eventsFile.get_df()\n",
    "\n",
    "print(\"Sample data: \\n\", eventsDF[:][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIDS Archive: Getting & Appending Scanning Runs\n",
    "\n",
    "One of the most important functions that a `BIDS Archive` enables in the context of RT-Cloud is working with `BIDS Runs`. From a `BIDS Archive`, you can get all the image data and metadata from a particular scanning run packaged into a `BIDS Run` using `getBidsRun`. The opposite operation, for when you already have data accumulated in a `BIDS Run` and want to append it to an existing archive (or create a new archive) is to append it to a `BIDS Archive` using `appendBidsRun`.\n",
    "\n",
    "For example, if you have a complete dataset that you want to test a new real-time experiment on, you can use `getBidsRun` to iterate over your entire dataset. From the run, you can stream the BIDS Incrementals in the run to RT-Cloud and the new experimental script you want to try out. Then, when you're running your new experiment in RT-Cloud for real, as `BIDS Incremental` files are streamed from the scanner to your script, you can create a new `BIDS Archive` in an empty folder on the computer running your script, build up your run as it happens in a `BidsRun`, and then add it to your archive all at once by calling `appendBidsRun` when your scanning run completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "td = tempfile.TemporaryDirectory()\n",
    "print('Temporary directory path:', td.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a run\n",
    "entityDict = {'subject': '01', 'task': 'languageproduction'}\n",
    "firstSubjectLangRun = archive.getBidsRun(**entityDict)\n",
    "    \n",
    "# If we append this run to an empty archive, the two archives will have the same data\n",
    "\n",
    "# Create new archive\n",
    "newArchive = BidsArchive(td.name)\n",
    "newArchive.appendBidsRun(firstSubjectLangRun)\n",
    "\n",
    "# Compare runs\n",
    "assert firstSubjectLangRun == newArchive.getBidsRun(**entityDict)\n",
    "\n",
    "# We can also build up runs from incrementals -- here, we'll fake a new run by modifying\n",
    "# the metadata to be for subject #2\n",
    "subjectTwoRun = BidsRun()\n",
    "for i in range(firstSubjectLangRun.numIncrementals()):\n",
    "    incremental = firstSubjectLangRun.getIncremental(i)\n",
    "    incremental.setMetadataField('subject', '02')\n",
    "    subjectTwoRun.appendIncremental(incremental)\n",
    "    \n",
    "assert '02' not in newArchive.getSubjects()\n",
    "newArchive.appendBidsRun(subjectTwoRun)\n",
    "assert '02' in newArchive.getSubjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "td.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIDS Incremental: Creating Incremental\n",
    "\n",
    "A `BIDS Incremental` has two primary components:\n",
    "1. A NIfTI image\n",
    "2. A metadata dictionary storing information about the image.\n",
    "\n",
    "It also has a few other components that are used when the `BIDS Incremental` is written to disk, and may be used by you for other purposes. Those are:\n",
    "1. The dataset description dictionary, which becomes the `dataset_description.json` in a BIDS Archive.\n",
    "2. The README string, which becomes the `README` file in a BIDS archive.\n",
    "3. The events dataframe, which becomes the `<file name entities>_events.tsv` file in a BIDS archive.\n",
    "\n",
    "To create a `BIDS Incremental`, only the image and the metadata dictionary are needed, and default versions of the other components are created if the `BIDS Incremental` is written to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reading from a BIDS-compliant dataset, all metadata\n",
    "should already be present, and using BIDS Archive methods\n",
    "to read the image and metadata is sufficient to create the\n",
    "incremental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the NIfTI image\n",
    "imageFile = archive.getImages(subject='01', run=1)[0]\n",
    "image = imageFile.get_image()\n",
    "\n",
    "# Get the metadata for the image\n",
    "metadata = archive.getSidecarMetadata(imageFile, includeEntities=True)\n",
    "\n",
    "# Create the BIDS Incremental\n",
    "incremental = BidsIncremental(image, metadata)\n",
    "print('Created Incremental: ', incremental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If converting from a DICOM image, sometimes extra work is needed to obtain all the metadata needed to create a valid BIDS-Incremental. This is because a BIDS-Incremental is a fully valid BIDS dataset itself, which has slightly different metadata than a DICOM. \n",
    "\n",
    "While RT-Cloud does its best to extract all possible metadata needed for BIDS from the DICOM image's metadata (e.g., it automatically extracts any BIDS entities from the DICOM's `ProtocolName` metadata field), sometimes you will have to manually specify fields for your experiment. The following example shows how these fields sometimes must be added by the user of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "td = tempfile.TemporaryDirectory()\n",
    "print('Temporary directory path:', td.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_NIFTI_PATH = os.path.join(td.name, TEMP_NIFTI_NAME)\n",
    "dicomPath = os.path.join(rootPath, DICOM_PATH)\n",
    "convertDicomFileToNifti(dicomPath, TEMP_NIFTI_PATH)\n",
    "image = readNifti(TEMP_NIFTI_PATH)\n",
    "\n",
    "dicomMetadata = getDicomMetadata(readDicomFromFile(dicomPath))\n",
    "\n",
    "try:\n",
    "    incremental = BidsIncremental(image, dicomMetadata)\n",
    "except MissingMetadataError as e:\n",
    "    print(\"-------- Metadata required for BIDS, unable to be extracted from DICOM --------\")\n",
    "    print(e)\n",
    "    print(\"----------------\")\n",
    "    # We can see that 'subject', 'suffix', and 'datatype' were not able to be \n",
    "    # extracted from the DICOM's metadata. \n",
    "    # This implies RT-Cloud was able to extract the other required fields \n",
    "    # (task, RepetitionTime, and EchoTime).\n",
    "    # Therefore, we'll only have to manually provide 'subject', 'suffix', \n",
    "    # and 'datatype' based on our knowledge of the experiment.\n",
    "    \n",
    "# Here, we'll pretend the subject is the 1st subject, the imaging methodology\n",
    "# was fMRI BOLD, and the datatype is func, representing a functional run\n",
    "dicomMetadata.update({'subject': '01', 'suffix': 'bold', 'datatype': 'func'})\n",
    "\n",
    "# Now, the incremental's creation will succeed\n",
    "incremental = BidsIncremental(image, dicomMetadata)\n",
    "\n",
    "print('Created Incremental:', incremental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "td.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIDS Incremental: Querying Incremental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `BIDS Incremental` is the basic unit of data transfer in RT-Cloud, and your scripts will often interact directly with an Incremental and the data within it. This part of the tutorial will show you how to obtain different parts of the Incremental's data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting, setting, and removing metadata\n",
    "fields = ['subject', 'task', 'RepetitionTime', 'ProtocolName']\n",
    "oldValues = {key: incremental.getMetadataField(key) for key in fields}\n",
    "\n",
    "print('-------- Getting Fields --------')\n",
    "for field in fields:\n",
    "    print(field + ': ' + str(incremental.getMetadataField(field)))\n",
    "    \n",
    "print('\\n-------- After Setting Fields --------')\n",
    "for field in fields:\n",
    "    incremental.setMetadataField(field, 'test')\n",
    "for field in fields:\n",
    "    print(field + ': ' + str(incremental.getMetadataField(field)))\n",
    "    \n",
    "print('\\n-------- Removing Fields --------')\n",
    "for field in fields:\n",
    "    # Note that required fields can only be changed, not removed\n",
    "    try:\n",
    "        incremental.removeMetadataField(field)\n",
    "    except RuntimeError as e:\n",
    "        print(str(e))\n",
    "for field in fields:\n",
    "    try:\n",
    "        print(field + ': ' + str(incremental.getMetadataField(field)))\n",
    "    except KeyError as e:\n",
    "        print(str(e))\n",
    "        \n",
    "# Restore original values\n",
    "for key, value in oldValues.items():\n",
    "    incremental.setMetadataField(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n-------- Full Metadata Dictionary --------')\n",
    "print(incremental.getImageMetadata())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Image and Image-Related Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to these methods, there are several properties that help extract particular entities or data having to do with the NIfTI image contained within the Incremental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities\n",
    "print('Suffix:', incremental.getSuffix())\n",
    "print('Datatype:', incremental.getDatatype())\n",
    "print('BIDS Entities:', incremental.getEntities())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Image properties\n",
    "print('Image dimensions:', incremental.getImageDimensions())\n",
    "print('\\nImage header:', incremental.getImageHeader())\n",
    "print('\\nImage data:', incremental.getImageData())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying BIDS Archive-Related Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each `BIDS Incremental` can also be made into a fully valid, on-disk BIDS Archive, there are also a variety of properties in the `BIDS Incremental` about how its data would be represented on disk in folders and files.\n",
    "\n",
    "When a `BIDS Archive` is created from a `BIDS Incremental`, several files are created in the archive, including the `README`, the events file, and the `dataset_description.json`. These have default values, but can all be manually modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_values = {'readme': incremental.readme, 'datasetMetadata': incremental.datasetMetadata, 'events': incremental.events}\n",
    "\n",
    "print('-------- Default Property Values --------')\n",
    "print('\\nREADME:', f'\"{incremental.readme}\"')\n",
    "print('\\nSource Dictionary for dataset_description.json:', incremental.datasetMetadata)\n",
    "print('\\nEvents File:', incremental.events)\n",
    "\n",
    "# modify the properties\n",
    "incremental.readme = 'Tutorial Dataset'\n",
    "\n",
    "incremental.datasetMetadata['Name'] = 'Tutorial Dataset'\n",
    "incremental.datasetMetadata['Authors'] = [\"Your Name\", \"Your Collaborator's Name\"]\n",
    "\n",
    "incremental.events = pd.DataFrame({'onset': 0.0, 'duration': 5.0, 'response_time':1.0}, index=[0])\n",
    "\n",
    "print('-------- Properties Post-Change --------')\n",
    "print('\\nREADME:', f'\"{incremental.readme}\"')\n",
    "print('\\nSource Dictionary for dataset_description.json:', incremental.datasetMetadata)\n",
    "print('\\nEvents File:', incremental.events)\n",
    "\n",
    "# restore previous values\n",
    "incremental.readme = old_values['readme']\n",
    "incremental.datasetMetadata = old_values['datasetMetadata']\n",
    "incremental.events = old_values['events']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file paths and names that will be created for the data in the archive can also be queried without actually writing the archive to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n-------- Directory Names and Paths --------')\n",
    "print('Dataset directory name:', incremental.getDatasetName())\n",
    "print('Data directory path:', incremental.getDataDirPath())\n",
    "\n",
    "print('\\n-------- File Names --------')\n",
    "print('Image file name:', incremental.getImageFileName())\n",
    "print('Metadata file name:', incremental.getMetadataFileName())\n",
    "print('Events file name:', incremental.getEventsFileName())\n",
    "\n",
    "print('\\n-------- File Paths --------')\n",
    "print('Image file path:', incremental.getImageFilePath())\n",
    "print('Metadata file path:', incremental.getMetadataFilePath())\n",
    "print('Events file path:', incremental.getEventsFilePath())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIDS Incremental: Writing to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key features of a `BIDS Incremental` is that it is also a valid, 1-image `BIDS Archive`. Thus, a `BIDS Incremental` can be written out to an archive on disk and navigated on the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "td = tempfile.TemporaryDirectory()\n",
    "print('Temporary directory path:', td.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental.writeToDisk(td.name)\n",
    "\n",
    "archiveFromIncremental = BidsArchive(td.name)\n",
    "print('Archive:', archiveFromIncremental)\n",
    "print('\\nBIDS Files in Archive from Incremental:', archiveFromIncremental.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "td.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIDS Incremental: Sending Over a Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BIDS Incrementals` are designed for transfer from one computer to another, often from the fMRI scanner room computer to the cloud for data processing. The process of preparing the Incremental for send and unpacking it on the other side is quite simple, using the Python `pickle` module.\n",
    "\n",
    "In this example, just the packing/unpacking process will be shown -- to actually send it over the network, pass the serialized object to your data transfer library of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize object\n",
    "pickledBuf = pickle.dumps(incremental)\n",
    "\n",
    "# Deserialize object\n",
    "unpickled = pickle.loads(pickledBuf)\n",
    "\n",
    "# Compare equality\n",
    "assert unpickled == incremental\n",
    "print('Unpickled:', unpickled, '\\nIncremental:', incremental)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtcloud",
   "language": "python",
   "name": "rtcloud"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
